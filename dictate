#!/usr/bin/env python3
"""Push-to-talk dictation using faster-whisper and evdev.

Modes:
  dictate              - push-to-talk daemon (hold key to record, clipboard output)
  dictate --serve      - background daemon (model stays loaded, listens on socket)
  dictate --once       - send record request to daemon, print transcription to stdout
"""

import argparse
import json
import os
import signal
import socket
import subprocess
import sys
import tomllib
from pathlib import Path
from selectors import DefaultSelector, EVENT_READ

import evdev
import numpy as np
import sounddevice as sd
from evdev import ecodes
from faster_whisper import WhisperModel

SAMPLE_RATE = 16000
CONFIG_PATH = os.path.expanduser("~/.config/dictate/config.toml")
SOCK_PATH = os.path.expanduser("~/.local/share/dictate/dictate.sock")
PID_PATH = os.path.expanduser("~/.local/share/dictate/dictate.pid")

DEFAULT_CONFIG = {
    "language": "en",
    "key": "RIGHTCTRL",
    "pre_buffer_secs": 1.0,
    "silence_secs": 3.0,
    "wait_secs": 10.0,
}


def load_config():
    """Load config from TOML file, falling back to defaults."""
    config = dict(DEFAULT_CONFIG)
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, "rb") as f:
            user = tomllib.load(f)
        config.update(user)
    return config


GLOBAL_HINTS_DIR = os.path.expanduser("~/.config/dictate/hints.d")
LOCAL_HINTS_DIR = ".dictate-hints.d"


def _read_hints_file(path):
    """Read words from a single hints file."""
    with open(path) as f:
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]


def _read_hints_dir(dirpath):
    """Read and merge all hint files in a directory."""
    words = []
    if not os.path.isdir(dirpath):
        return words
    for name in sorted(os.listdir(dirpath)):
        fpath = os.path.join(dirpath, name)
        if os.path.isfile(fpath):
            words.extend(_read_hints_file(fpath))
    return words


def load_hints():
    """Load vocabulary hints. Merges: ~/.config/dictate/hints.d/* + .dictate-hints.d/* in CWD.
    Returns initial_prompt string or None."""
    words = []
    words.extend(_read_hints_dir(GLOBAL_HINTS_DIR))
    words.extend(_read_hints_dir(os.path.join(os.getcwd(), LOCAL_HINTS_DIR)))
    # deduplicate preserving order
    seen = set()
    unique = []
    for w in words:
        if w.lower() not in seen:
            seen.add(w.lower())
            unique.append(w)
    if not unique:
        return None
    prompt = "Technical terms: " + ", ".join(unique) + "."
    return prompt


def find_audio_device():
    """Find the best input device, preferring pipewire."""
    devices = sd.query_devices()
    for i, dev in enumerate(devices):
        if dev['max_input_channels'] > 0 and 'pipewire' in dev['name'].lower():
            return i
    return None


def has_cuda():
    try:
        import ctranslate2
        return len(ctranslate2.get_supported_compute_types("cuda")) > 0
    except Exception:
        return False


def pick_defaults():
    if has_cuda():
        return "cuda", "int8", "medium"
    else:
        return "cpu", "int8", "small"


def find_keyboards():
    found = []
    devices = [evdev.InputDevice(path) for path in evdev.list_devices()]
    for dev in devices:
        caps = dev.capabilities(verbose=True)
        for (etype, _), codes in caps.items():
            if etype == "EV_KEY":
                key_names = [name for name, _ in codes]
                if "KEY_A" in key_names and "KEY_ENTER" in key_names:
                    found.append(dev)
                    break
    return found


def list_devices():
    devices = [evdev.InputDevice(path) for path in evdev.list_devices()]
    for dev in devices:
        if ecodes.EV_KEY in dev.capabilities():
            print(f"  {dev.path}: {dev.name}")


def calibrate_mic(audio_dev):
    """Measure ambient noise for 0.5s and return speech threshold."""
    calibration_rms = []

    def cal_cb(indata, frames, time_info, status):
        calibration_rms.append(np.sqrt(np.mean(indata ** 2)))

    stream = sd.InputStream(
        samplerate=SAMPLE_RATE, channels=1, dtype="float32",
        callback=cal_cb, device=audio_dev,
    )
    stream.start()
    sd.sleep(500)
    stream.stop()
    stream.close()

    ambient = np.mean(calibration_rms) if calibration_rms else 0.01
    threshold = ambient * 1.5 + 0.01
    # Cap threshold — if ambient was noisy during calibration, don't make it impossible to speak
    if threshold > 0.15:
        print(f"  WARNING: high ambient noise (rms={ambient:.4f}), capping threshold at 0.15 (was {threshold:.4f})")
        print(f"  Try a better microphone or move to a quieter environment.")
        threshold = 0.15
    elif ambient < 0.001:
        print(f"  WARNING: very low ambient level (rms={ambient:.4f}), mic may not be picking up audio.")
        print(f"  Check your microphone or try: dictate --list-devices")
    print(f"  ambient_rms={ambient:.4f}, threshold={threshold:.4f}")
    return threshold


STOP_FLAG = os.path.expanduser("~/.local/share/dictate/stop")


def record_until_silence(audio_dev, speech_threshold, silence_secs=3, wait_secs=10):
    """Record audio. Stop after silence_secs of post-speech silence,
    wait_secs of no speech, or STOP_FLAG file appearing."""
    import time as _time

    # Clear any stale stop flag
    if os.path.exists(STOP_FLAG):
        os.unlink(STOP_FLAG)

    audio_buffer = []
    is_recording = True
    speech_detected = False
    last_speech_time = _time.monotonic()
    start_time = _time.monotonic()

    def audio_cb(indata, frames, time_info, status):
        nonlocal speech_detected, last_speech_time, is_recording
        audio_buffer.append(indata.copy())
        rms = np.sqrt(np.mean(indata ** 2))
        now = _time.monotonic()
        if rms > speech_threshold:
            speech_detected = True
            last_speech_time = now
        elif speech_detected:
            if now - last_speech_time >= silence_secs:
                is_recording = False
        else:
            if now - start_time >= wait_secs:
                is_recording = False

    stream = sd.InputStream(
        samplerate=SAMPLE_RATE, channels=1, dtype="float32",
        callback=audio_cb, device=audio_dev,
    )
    stream.start()

    try:
        while is_recording:
            sd.sleep(100)
            # Check for manual stop
            if os.path.exists(STOP_FLAG):
                os.unlink(STOP_FLAG)
                speech_detected = True  # treat as valid recording
                break
    except KeyboardInterrupt:
        pass
    finally:
        stream.stop()
        stream.close()

    if not audio_buffer or not speech_detected:
        return None

    return np.concatenate(audio_buffer, axis=0).flatten()


# ── Daemon (--serve) ──────────────────────────────────────────────

def serve(model, language, audio_dev, config, initial_prompt, verbose=False):
    """Run as a background daemon. Listen on Unix socket for record requests."""
    speech_threshold = calibrate_mic(audio_dev)
    print(f"Mic calibrated. speech_threshold={speech_threshold:.4f}")
    if initial_prompt:
        print(f"Vocabulary hints loaded.")

    # Persistent audio stream with rolling pre-buffer
    from collections import deque
    pre_buffer_secs = config.get("pre_buffer_secs", 1.0)
    pre_buffer_frames = int(pre_buffer_secs * SAMPLE_RATE / 1024) + 1
    pre_buffer = deque(maxlen=pre_buffer_frames)

    def prebuf_callback(indata, frames, time_info, status):
        pre_buffer.append(indata.copy())

    prebuf_stream = sd.InputStream(
        samplerate=SAMPLE_RATE, channels=1, dtype="float32",
        callback=prebuf_callback, device=audio_dev, blocksize=1024,
    )
    prebuf_stream.start()
    print("Pre-buffer stream started (1s rolling buffer).")

    Path(SOCK_PATH).parent.mkdir(parents=True, exist_ok=True)
    if os.path.exists(SOCK_PATH):
        os.unlink(SOCK_PATH)

    with open(PID_PATH, "w") as f:
        f.write(str(os.getpid()))

    server = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    server.bind(SOCK_PATH)
    server.listen(1)

    def cleanup(signum=None, frame=None):
        prebuf_stream.stop()
        prebuf_stream.close()
        server.close()
        for p in (SOCK_PATH, PID_PATH):
            if os.path.exists(p):
                os.unlink(p)
        sys.exit(0)

    signal.signal(signal.SIGTERM, cleanup)
    signal.signal(signal.SIGINT, cleanup)

    print(f"Daemon ready. PID={os.getpid()}, socket={SOCK_PATH}")
    print("Waiting for requests...\n")

    try:
        while True:
            conn, _ = server.accept()
            try:
                chunks = []
                while True:
                    chunk = conn.recv(4096)
                    if not chunk:
                        break
                    chunks.append(chunk)
                data = b"".join(chunks).decode().strip()
                req = json.loads(data) if data else {}
                silence = req.get("silence_secs", config.get("silence_secs", 3))
                wait = req.get("wait_secs", config.get("wait_secs", 10))

                if verbose:
                    print(f"\n[debug] request: {len(data)} bytes, silence={silence}s, wait={wait}s")

                def send_status(status):
                    conn.sendall((json.dumps({"status": status}) + "\n").encode())

                # Grab pre-buffer snapshot before recording starts
                pre_audio = list(pre_buffer)
                if verbose:
                    pre_dur = sum(len(c) for c in pre_audio) / SAMPLE_RATE if pre_audio else 0
                    print(f"[debug] pre-buffer: {len(pre_audio)} chunks, {pre_dur:.2f}s")

                send_status("recording")
                print("● Recording...", end="", flush=True)
                audio = record_until_silence(audio_dev, speech_threshold,
                                             silence_secs=silence, wait_secs=wait)

                # Prepend pre-buffer to captured audio
                if audio is not None and pre_audio:
                    pre = np.concatenate(pre_audio, axis=0).flatten()
                    audio = np.concatenate([pre, audio])

                if audio is None:
                    print(" (no speech)\n")
                    conn.sendall((json.dumps({"text": ""}) + "\n").encode())
                    continue

                duration = len(audio) / SAMPLE_RATE
                rms = np.sqrt(np.mean(audio ** 2))
                send_status("transcribing")
                print(f" [{duration:.1f}s] transcribing...", end="", flush=True)
                if verbose:
                    print(f"\n[debug] audio: {duration:.2f}s, rms={rms:.4f}, threshold={speech_threshold:.4f}")

                lang = req.get("language", language)
                req_prompt = req.get("initial_prompt", initial_prompt)
                if verbose:
                    print(f"[debug] language={lang}")
                    print(f"[debug] initial_prompt={'[' + str(len(req_prompt)) + ' chars] ' + req_prompt[:80] + '...' if req_prompt else 'None'}")

                import time as _time
                t0 = _time.monotonic()
                segments, info = model.transcribe(
                    audio, language=lang,
                    initial_prompt=req_prompt,
                    hallucination_silence_threshold=2,
                )
                text = " ".join(seg.text for seg in segments).strip()
                t1 = _time.monotonic()

                if verbose:
                    print(f"[debug] transcribe took {t1-t0:.2f}s, detected_language={info.language} (p={info.language_probability:.2f})")

                print(f" done.\n> {text}\n")
                conn.sendall((json.dumps({"text": text}) + "\n").encode())
            except Exception as e:
                print(f" error: {e}\n")
                try:
                    conn.sendall(json.dumps({"error": str(e)}).encode())
                except Exception:
                    pass
            finally:
                conn.close()
    finally:
        cleanup()


# ── Client (--once) ───────────────────────────────────────────────

def client_once(language, status_callback=None):
    """Send a record request to the daemon and print the result.
    status_callback(status_str) is called for each status update."""
    if not os.path.exists(SOCK_PATH):
        print("Daemon not running. Start it with: dictate --serve", file=sys.stderr)
        sys.exit(1)

    # Load project-local hints from CWD
    initial_prompt = load_hints()

    sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    sock.connect(SOCK_PATH)
    req = {"language": language}
    if initial_prompt:
        req["initial_prompt"] = initial_prompt
    sock.sendall(json.dumps(req).encode())
    # Signal we're done sending
    sock.shutdown(socket.SHUT_WR)

    # Read newline-delimited JSON messages
    buf = b""
    text = ""
    while True:
        chunk = sock.recv(4096)
        if not chunk:
            break
        buf += chunk
        while b"\n" in buf:
            line, buf = buf.split(b"\n", 1)
            msg = json.loads(line.decode())
            if "status" in msg and status_callback:
                status_callback(msg["status"])
            elif "status" in msg:
                print(msg["status"], file=sys.stderr, flush=True)
            if "text" in msg:
                text = msg["text"]
    sock.close()

    if text:
        print(text)
    return text


# ── Push-to-talk (default) ───────────────────────────────────────

def push_to_talk(model, language, key_code, key_name, keyboards, audio_dev, initial_prompt):
    audio_buffer = []
    is_recording = False

    def audio_callback(indata, frames, time_info, status):
        if is_recording:
            audio_buffer.append(indata.copy())

    stream = sd.InputStream(
        samplerate=SAMPLE_RATE, channels=1, dtype="float32",
        callback=audio_callback, device=audio_dev,
    )
    stream.start()

    print("Listening on:")
    for kbd in keyboards:
        print(f"  {kbd.name} ({kbd.path})")
    print(f"Hold [{key_name}] to record, release to transcribe.")
    print(f"Ctrl+C to quit.\n")

    selector = DefaultSelector()
    for kbd in keyboards:
        selector.register(kbd, EVENT_READ)

    try:
        while True:
            for sel_key, _ in selector.select():
                dev = sel_key.fileobj
                for event in dev.read():
                    if event.type != ecodes.EV_KEY:
                        continue
                    key_event = evdev.categorize(event)
                    if key_event.scancode != key_code:
                        continue

                    if key_event.keystate == evdev.KeyEvent.key_down and not is_recording:
                        audio_buffer.clear()
                        is_recording = True
                        print("● Recording...", end="", flush=True)

                    elif key_event.keystate == evdev.KeyEvent.key_up and is_recording:
                        is_recording = False

                        if not audio_buffer:
                            print(" (no audio captured)\n")
                            continue

                        audio = np.concatenate(audio_buffer, axis=0).flatten()
                        duration = len(audio) / SAMPLE_RATE
                        print(f" [{duration:.1f}s] transcribing...", end="", flush=True)

                        segments, _ = model.transcribe(
                            audio, language=language,
                            initial_prompt=initial_prompt,
                            hallucination_silence_threshold=2,
                        )
                        text = " ".join(seg.text for seg in segments).strip()

                        if text:
                            subprocess.run(["wl-copy", text])
                            print(f" done. [copied]\n> {text}\n")
                        else:
                            print(" (no speech detected)\n")

    except KeyboardInterrupt:
        print("\nStopping.")
    finally:
        stream.stop()
        stream.close()


def main():
    config = load_config()
    hw_device, hw_compute, hw_model = pick_defaults()

    parser = argparse.ArgumentParser(description="Push-to-talk dictation")
    parser.add_argument("--key", default=config.get("key", "RIGHTCTRL"),
        help="Trigger key (default: from config)")
    parser.add_argument("--model", default=hw_model,
        help=f"Whisper model (default: {hw_model})")
    parser.add_argument("--device", help="Input device path")
    parser.add_argument("--list-devices", action="store_true")
    parser.add_argument("--language", default=config.get("language", "en"),
        help="Language (default: from config)")
    parser.add_argument("--cpu", action="store_true", help="Force CPU")
    parser.add_argument("--serve", action="store_true",
        help="Run as background daemon (model stays loaded)")
    parser.add_argument("--once", action="store_true",
        help="Send record request to daemon, print result, exit")
    parser.add_argument("--stop", action="store_true",
        help="Stop the running daemon")
    parser.add_argument("--stop-recording", action="store_true",
        help="Signal the daemon to stop current recording immediately")
    parser.add_argument("--verbose", "-v", action="store_true",
        help="Enable debug logging")
    args = parser.parse_args()

    if args.list_devices:
        list_devices()
        return

    # --once: just talk to daemon
    if args.once:
        client_once(args.language)
        return

    # --stop-recording: signal daemon to stop current recording
    if args.stop_recording:
        Path(STOP_FLAG).parent.mkdir(parents=True, exist_ok=True)
        Path(STOP_FLAG).touch()
        return

    # --stop: kill daemon
    if args.stop:
        if os.path.exists(PID_PATH):
            pid = int(open(PID_PATH).read().strip())
            os.kill(pid, signal.SIGTERM)
            print(f"Stopped daemon (PID {pid}).")
        else:
            print("No daemon running.")
        return

    # Hardware setup
    if args.cpu:
        device, compute = "cpu", "int8"
    else:
        device, compute = hw_device, hw_compute

    audio_dev = find_audio_device()

    initial_prompt = load_hints()

    print(f"Loading whisper model '{args.model}' on {device} ({compute})...")
    model = WhisperModel(args.model, device=device, compute_type=compute)

    # --serve: daemon mode
    if args.serve:
        serve(model, args.language, audio_dev, config, initial_prompt, args.verbose)
        return

    # Default: push-to-talk
    key_name = f"KEY_{args.key.upper()}"
    key_code = getattr(ecodes, key_name, None)
    if key_code is None:
        print(f"Unknown key: {key_name}")
        sys.exit(1)

    if args.device:
        keyboards = [evdev.InputDevice(args.device)]
    else:
        keyboards = find_keyboards()
        if not keyboards:
            print("No keyboard found. Try --list-devices and --device.")
            sys.exit(1)

    push_to_talk(model, args.language, key_code, args.key.upper(), keyboards, audio_dev, initial_prompt)


if __name__ == "__main__":
    main()
